{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pygame\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# Specify the folder path containing the .npy files\n",
    "folder_path = \"Quick_Draw_Data\"\n",
    "\n",
    "# List the .npy files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "file_paths = [file_name for file_name in file_names if file_name.endswith('.npy')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# Specify the folder path containing the .npy files\n",
    "folder_path = \"Quick_Draw_Data\"\n",
    "\n",
    "# List the .npy files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "file_paths = [file_name for file_name in file_names if file_name.endswith('.npy')]\n",
    "\n",
    "# Load the .npy files and store them in a dictionary\n",
    "data_dict = {}\n",
    "samples=10000\n",
    "for i in range(40):\n",
    "    data = np.load(folder_path + \"/\" + file_paths[i])[:samples]\n",
    "    # label = np.empty((data.shape[0],1),dtype=object)\n",
    "    # astr = file_paths[i][18:-4]\n",
    "    label = np.empty((data.shape[0],1),dtype=float)\n",
    "    astr=i\n",
    "    for k in range(label.shape[0]):\n",
    "        for j in range(label.shape[1]):\n",
    "            label[k][j] = astr\n",
    "    data = np.hstack((data,label))\n",
    "    data_dict[i] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'airplane', 1: 'apple', 2: 'axe', 3: 'basketball', 4: 'bed', 5: 'bicycle', 6: 'bowtie', 7: 'bread', 8: 'bucket', 9: 'butterfly', 10: 'cactus', 11: 'cake', 12: 'candle', 13: 'carrot', 14: 'circle', 15: 'clock', 16: 'cloud', 17: 'crayon', 18: 'crown', 19: 'cup', 20: 'diamond', 21: 'donut', 22: 'ear', 23: 'envelope', 24: 'eye', 25: 'eyeglasses', 26: 'finger', 27: 'fish', 28: 'flower', 29: 'fork', 30: 'hammer', 31: 'hat', 32: 'headphones', 33: 'hockey stick', 34: 'ice cream', 35: 'key', 36: 'leaf', 37: 'moon', 38: 'mountain', 39: 'mushroom'}\n"
     ]
    }
   ],
   "source": [
    "look_up_table={}\n",
    "for i in range(40):\n",
    "    look_up_table[i] = file_paths[i][18:-4]\n",
    "print(look_up_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify the index of the sample you want to plot\n",
    "sample_index = 0\n",
    "\n",
    "# Access the sample from the data dictionary\n",
    "sample = data_dict[0][100, :-1].astype(float)  # Exclude the label from the sample\n",
    "\n",
    "# Reshape the sample to the image dimensions (assuming it's a 28x28 image)\n",
    "image = sample.reshape((28, 28))\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 320000\n",
      "Test set size: 80000\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "test = []\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    # print(value[0])\n",
    "    train.extend(value[:8000])  # First 8000 samples for training\n",
    "    test.extend(value[8000:10000])  # Remaining 2000 samples for testing\n",
    "print(\"Train set size:\", len(train))\n",
    "print(\"Test set size:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 40\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,shuffle=True)\n",
    "test_loader= torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3,padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, 120)  \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.835\n",
      "Epoch [2/20], Loss: 0.613\n",
      "Epoch [3/20], Loss: 0.562\n",
      "Epoch [4/20], Loss: 0.532\n",
      "Epoch [5/20], Loss: 0.512\n",
      "Epoch [6/20], Loss: 0.499\n",
      "Epoch [7/20], Loss: 0.490\n",
      "Epoch [8/20], Loss: 0.480\n",
      "Epoch [9/20], Loss: 0.473\n",
      "Epoch [10/20], Loss: 0.467\n",
      "Epoch [11/20], Loss: 0.461\n",
      "Epoch [12/20], Loss: 0.455\n",
      "Epoch [13/20], Loss: 0.452\n",
      "Epoch [14/20], Loss: 0.448\n",
      "Epoch [15/20], Loss: 0.443\n",
      "Epoch [16/20], Loss: 0.440\n",
      "Epoch [17/20], Loss: 0.435\n",
      "Epoch [18/20], Loss: 0.432\n",
      "Epoch [19/20], Loss: 0.429\n",
      "Epoch [20/20], Loss: 0.428\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "outputs = torch.tensor([])\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs= data[:,:784].float()\n",
    "        inputs = inputs.view(-1, 1, 28, 28)  # Reshape the input tensor\n",
    "        # inputs=inputs.to(device)\n",
    "        labels=data[:,784]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    # print statistics at the end of each epoch\n",
    "    print(f'Epoch [{epoch + 1}/20], Loss: {running_loss / len(train_loader):.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './quick_draw_cnn.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 80000 test images: 85.4925 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for data in test_loader:\n",
    "        inputs= data[:,:784].float()\n",
    "        inputs = inputs.view(-1, 1, 28, 28)  # Reshape the input tensor\n",
    "        # inputs=inputs.to(device)\n",
    "        labels=data[:,784]\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {len(test)} test images: {100 * correct / total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: airplane is 88.5 %\n",
      "Accuracy for class: apple is 92.8 %\n",
      "Accuracy for class: axe   is 76.5 %\n",
      "Accuracy for class: basketball is 82.8 %\n",
      "Accuracy for class: bed   is 86.8 %\n",
      "Accuracy for class: bicycle is 92.6 %\n",
      "Accuracy for class: bowtie is 85.8 %\n",
      "Accuracy for class: bread is 72.7 %\n",
      "Accuracy for class: bucket is 88.0 %\n",
      "Accuracy for class: butterfly is 88.4 %\n",
      "Accuracy for class: cactus is 84.5 %\n",
      "Accuracy for class: cake  is 88.8 %\n",
      "Accuracy for class: candle is 86.0 %\n",
      "Accuracy for class: carrot is 85.4 %\n",
      "Accuracy for class: circle is 90.0 %\n",
      "Accuracy for class: clock is 92.3 %\n",
      "Accuracy for class: cloud is 83.5 %\n",
      "Accuracy for class: crayon is 76.7 %\n",
      "Accuracy for class: crown is 89.8 %\n",
      "Accuracy for class: cup   is 82.1 %\n",
      "Accuracy for class: diamond is 84.8 %\n",
      "Accuracy for class: donut is 91.8 %\n",
      "Accuracy for class: ear   is 81.7 %\n",
      "Accuracy for class: envelope is 95.0 %\n",
      "Accuracy for class: eye   is 89.4 %\n",
      "Accuracy for class: eyeglasses is 86.2 %\n",
      "Accuracy for class: finger is 77.6 %\n",
      "Accuracy for class: fish  is 88.0 %\n",
      "Accuracy for class: flower is 87.4 %\n",
      "Accuracy for class: fork  is 81.1 %\n",
      "Accuracy for class: hammer is 80.1 %\n",
      "Accuracy for class: hat   is 84.4 %\n",
      "Accuracy for class: headphones is 91.8 %\n",
      "Accuracy for class: hockey stick is 91.2 %\n",
      "Accuracy for class: ice cream is 87.8 %\n",
      "Accuracy for class: key   is 79.2 %\n",
      "Accuracy for class: leaf  is 79.7 %\n",
      "Accuracy for class: moon  is 67.7 %\n",
      "Accuracy for class: mountain is 90.0 %\n",
      "Accuracy for class: mushroom is 91.2 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in look_up_table.values()}\n",
    "total_pred = {classname: 0 for classname in look_up_table.values()}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs= data[:,:784].float()\n",
    "        inputs = inputs.view(-1, 1, 28, 28)  # Reshape the input tensor\n",
    "        # inputs=inputs.to(device)\n",
    "        labels=data[:,784]    \n",
    "        outputs = net(inputs)   \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            # if label.item() == 0:\n",
    "                # print(\"predicted: \", prediction.item())\n",
    "            if label == prediction:\n",
    "                correct_pred[look_up_table[label.item()]] += 1\n",
    "            total_pred[look_up_table[label.item()]] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    total_count = total_pred[classname]\n",
    "    if total_count == 0:\n",
    "        accuracy = 0.0  # If no instances of this class in test dataset, set accuracy to 0\n",
    "    else:\n",
    "        accuracy = 100 * float(correct_count) / total_count\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load('./quick_draw_cnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved at: /mnt/c/Users/rushi/Desktop/IIT_Bombay/Academics/Sem_4/ML/Skribble_Buddy/drawn_image.jpg\n"
     ]
    }
   ],
   "source": [
    "# Initialize PyGame\n",
    "pygame.init()\n",
    "\n",
    "# Set up the drawing window\n",
    "canvas_width, canvas_height = 600, 600\n",
    "canvas = pygame.display.set_mode((canvas_width, canvas_height))\n",
    "\n",
    "random_number = random.randint(0, 39)\n",
    "\n",
    "pygame.display.set_caption(f\"Draw {look_up_table[random_number]} Press ctrl+c to clear canvas & close to save\")\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Function to clear the canvas\n",
    "def clear_canvas():\n",
    "    canvas.fill(WHITE)\n",
    "    pygame.display.flip()  # Update the display to show the cleared canvas\n",
    "\n",
    "# Game loop\n",
    "drawing = False\n",
    "last_pos = (0, 0)\n",
    "canvas.fill(WHITE)\n",
    "\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "        elif event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            drawing = True\n",
    "            last_pos = event.pos\n",
    "\n",
    "        elif event.type == pygame.MOUSEBUTTONUP:\n",
    "            drawing = False\n",
    "\n",
    "        elif event.type == pygame.MOUSEMOTION:\n",
    "            if drawing:\n",
    "                mouse_pos = event.pos\n",
    "                pygame.draw.line(canvas, BLACK, last_pos, mouse_pos, 5)  # Draw line\n",
    "                last_pos = mouse_pos\n",
    "\n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_c and pygame.key.get_mods() & pygame.KMOD_CTRL:\n",
    "                clear_canvas()\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Save the canvas as an image\n",
    "image_path = \"drawn_image.jpg\"\n",
    "pygame.image.save(canvas, image_path)\n",
    "\n",
    "# Clean up\n",
    "pygame.quit()\n",
    "\n",
    "print(f\"Image saved at: {os.path.abspath(image_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "#Convert to grayscale\n",
    "gray_image = image.convert('L')\n",
    "# print(np.min(gray_image))\n",
    "\n",
    "# Resize the image to 28x28 using skimage\n",
    "resized_image = resize(np.array(gray_image), (28, 28), anti_aliasing=True)\n",
    "\n",
    "# Convert the image to a numpy array and normalize pixel values\n",
    "\n",
    "bitmap = (np.ones(resized_image.shape)-np.array(resized_image))\n",
    "max_val = np.max(bitmap)\n",
    "if max_val != 0:\n",
    "    bitmap = bitmap*255/max_val\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the numpy array as an image\n",
    "\n",
    "plt.imshow(bitmap, cmap='gray')  # cmap='gray' for grayscale images\n",
    "\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  headphones\n"
     ]
    }
   ],
   "source": [
    "bitmap_tensor = torch.Tensor(bitmap).float()  # Convert to float tensor\n",
    "bitmap_input = bitmap_tensor.view(-1, 1, 28, 28)   # Flatten to 1D tensor\n",
    "\n",
    "# Pass the tensor to the network\n",
    "outputs = net(bitmap_input)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', look_up_table[predicted.item()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
